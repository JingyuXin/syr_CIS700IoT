{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "fixlen_path = '/home/jxin05/BBMAS_Data/fixlen_data/'# data with length 2mins is in this folder, each sample is a .csv file\n",
    "\n",
    "def remove_col(df):\n",
    "    return df.drop(['EID','time','time_in_ms'], axis =1)\n",
    "\n",
    "def train_test_split(path):\n",
    "    file_list = os.listdir(path)\n",
    "    idx = np.random.permutation(len(file_list))\n",
    "    train_idx = idx[:int(len(file_list)*0.8)]\n",
    "    test_idx = idx[int(len(file_list)*0.8):]\n",
    "    trainfile = []\n",
    "    testfile = []\n",
    "    for i in train_idx:\n",
    "        trainfile.append(file_list[i])\n",
    "    for i in test_idx:\n",
    "        testfile.append(file_list[i])\n",
    "    return trainfile,testfile\n",
    "\n",
    "def mix_data(path, filelist, fake_number, replace_time, replace_size = 200):\n",
    "    pick_dict = {}\n",
    "    mixed_data = []\n",
    "    count = 0\n",
    "    while(count < fake_number):\n",
    "        pick = np.random.randint(3, len(filelist)-1)\n",
    "        pick_dict[count] = []\n",
    "        #print(\"creating fake sample from \" + filelist[pick])\n",
    "        target = pd.read_csv(path + filelist[pick])\n",
    "        mix = np.asarray(remove_col(target).values)\n",
    "        for j in range(replace_time):\n",
    "            start = np.random.randint(3,len(target)/replace_size-1)\n",
    "            while(start in pick_dict[count]):\n",
    "                start = np.random.randint(3,len(target)/replace_size-1)\n",
    "            pick_dict[count].append(start)\n",
    "            replace_clip = take_from_others(path, filelist,replace_size, pick)\n",
    "            mix[start*replace_size:(start+1)*replace_size, :] = replace_clip\n",
    "        mixed_data.append(mix)\n",
    "        #print(\"clips replaced: {}\".format(np.sort(pick_dict[count])))\n",
    "        count+=1\n",
    "        #print(\"**********\")\n",
    "    return mixed_data, pick_dict \n",
    "            \n",
    "def take_from_others(path, filelist, replace_size, pick):\n",
    "    takefrom = np.random.randint(0,len(filelist))\n",
    "    while(takefrom == pick):\n",
    "        takefrom = np.random.randint(0,len(filelist))\n",
    "    target_df = pd.read_csv(path+filelist[takefrom])\n",
    "    if(len(target_df) != 12000):\n",
    "        print(filelist[takefrom])\n",
    "    start_idx = np.random.randint(0,len(target_df)-replace_size)\n",
    "    replace_clip = target_df.iloc[start_idx:start_idx+replace_size]\n",
    "    replace_clip = remove_col(replace_clip)\n",
    "    return np.asarray(replace_clip.values)\n",
    "\n",
    "def load_original_data(path,filelist, fix_len):# modify it !!!!\n",
    "    X = np.zeros((len(filelist), 3, fix_len))\n",
    "    Y = np.zeros((len(filelist), 60))    \n",
    "    for i in range(len(filelist)):\n",
    "        x = remove_col(pd.read_csv(path+filelist[i])).values\n",
    "        X[i,:,:] = x.T\n",
    "    return X,Y\n",
    "\n",
    "def smooth(mixed_data, pick_dict, window = 3):\n",
    "    res = mixed_data\n",
    "    for i in range(len(mixed_data)):\n",
    "        for v in pick_dict[i]:\n",
    "            #print(v)\n",
    "            start = v*200-10\n",
    "            end = v*200+10\n",
    "            data_x = mixed_data[i][start:end,0]\n",
    "            #print(data_x)\n",
    "            data_y = mixed_data[i][start:end,1]\n",
    "            data_z = mixed_data[i][start:end,2]\n",
    "            smoothed_x = ExpMovingAverage(data_x, window)\n",
    "            #print(smoothed_x)\n",
    "            smoothed_y = ExpMovingAverage(data_y, window)\n",
    "            smoothed_z = ExpMovingAverage(data_z, window)\n",
    "            res[i][start+window+1:end-window,0] = smoothed_x[window+1:-window]\n",
    "            #print(res[i][start+window+1:end-window,0])\n",
    "            res[i][start+window+1:end-window,1] = smoothed_y[window+1:-window]\n",
    "            res[i][start+window+1:end-window,2] = smoothed_z[window+1:-window]\n",
    "    return res\n",
    "def ExpMovingAverage(array, window):\n",
    "    weights = np.exp(np.linspace(-1., 0., window))\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    a = np.convolve(array, weights, mode='full')[:len(array)]\n",
    "    a[:window] = a[window]\n",
    "    return a\n",
    "def make_fake_dataset(mixed_data, pick_dict):\n",
    "    sample_len = mixed_data[0].shape[0]\n",
    "    num_sample = len(mixed_data)\n",
    "    X = np.zeros((num_sample, mixed_data[0].shape[1], sample_len))\n",
    "    Y = np.zeros((num_sample, 60))\n",
    "    for i in range(len(mixed_data)):\n",
    "        X[i,:,:] = mixed_data[i].T\n",
    "        for v in pick_dict[i]:\n",
    "            Y[i,v] = 1\n",
    "    return X,Y\n",
    "def transformation(x):\n",
    "    if(len(x.shape) == 3):\n",
    "        inputslist = []\n",
    "        for i in range(x.shape[0]):\n",
    "            inputs_x = torch.from_numpy(x[i,0,:]).float()\n",
    "            inputs_x = inputs_x.view(60,-1)\n",
    "            inputs_y = torch.from_numpy(x[i,1,:]).float()\n",
    "            inputs_y = inputs_y.view(60,-1)\n",
    "            inputs_z = torch.from_numpy(x[i,2,:]).float()\n",
    "            inputs_z = inputs_z.view(60,-1)\n",
    "            inputs = torch.stack([inputs_x, inputs_y, inputs_z])\n",
    "            inputs = inputs.transpose(0,1)\n",
    "            inputslist.append(inputs)\n",
    "        res = torch.stack(inputslist)\n",
    "        res = res.view(x.shape[0], 60, 1, 3, 200)\n",
    "        return res\n",
    "    else:\n",
    "        inputs_x = torch.from_numpy(x[0,:]).float()\n",
    "        inputs_x = inputs_x.view(60,-1)\n",
    "        inputs_y = torch.from_numpy(x[1,:]).float()\n",
    "        inputs_y = inputs_y.view(60,-1)\n",
    "        inputs_z = torch.from_numpy(x[2,:]).float()\n",
    "        inputs_z = inputs_z.view(60,-1)\n",
    "        inputs = torch.stack([inputs_x, inputs_y, inputs_z])\n",
    "        inputs = inputs.transpose(0,1)\n",
    "        inputs = inputs.view(-1, 60, 1, 3, 200)\n",
    "        return inputs\n",
    "def take_batch(batch_size, X, Y):\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    for i in range(0,X.shape[0]-batch_size+1, batch_size):\n",
    "        excerpt = indices[i:i + batch_size]\n",
    "        yield X[excerpt], Y[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cnn_lstm1(nn.Module):\n",
    "    \n",
    "    def __init__(self,drop_prob = 0.3, n_class = 2, n_layer = 1):\n",
    "        super(cnn_lstm1, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,32,(2,8), stride = (1,4))\n",
    "        self.conv2 = nn.Conv2d(32,64, (2,4), stride = (1,2))\n",
    "        self.conv3 = nn.Conv1d(64,32, 4, stride = 2)\n",
    "        self.lstm = nn.LSTM(320, 128, num_layers = 1, batch_first = True)\n",
    "        self.fc1 = nn.Linear(128, 32)\n",
    "        self.fc2 = nn.Linear(32,n_class)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.outlayer = nn.Softmax(dim = 2)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        inputs = transformation(x)\n",
    "        batch_size, timesteps, C, H, W = inputs.size()\n",
    "        inputs = inputs.view(batch_size*timesteps,C,H,W)\n",
    "        inputs = self.conv1(inputs)\n",
    "        inputs = F.relu(inputs)\n",
    "        inputs = self.conv2(inputs)\n",
    "        inputs = F.relu(inputs)\n",
    "        inputs = inputs.squeeze()\n",
    "        inputs = self.conv3(inputs)\n",
    "        inputs = F.relu(inputs)\n",
    "        \n",
    "        inputs = inputs.view(batch_size, timesteps, -1)\n",
    "        inputs, hidden = self.lstm(inputs,hidden)\n",
    "        inputs = self.dropout(inputs)\n",
    "        inputs = self.fc1(inputs)\n",
    "        inputs = self.dropout(inputs)\n",
    "        inputs = self.fc2(inputs)\n",
    "        out = self.outlayer(inputs)\n",
    "        \n",
    "        if(batch_size != 1):\n",
    "            #print(\"doing transpose\")\n",
    "            out = out.transpose(1,2)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(1, batch_size, 128).zero_(),\n",
    "                      weight.new(1, batch_size, 128).zero_())\n",
    "        return hidden\n",
    "\n",
    "def init_weights1(m):\n",
    "    if type(m) == nn.LSTM:\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    elif type(m) == nn.Conv2d or type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch_m2m(net, X,Y, batch_size = 20, epochs = 20, lr = 0.01):\n",
    "    opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        train_loss = []\n",
    "        net.train()\n",
    "        for x,y in take_batch(batch_size, X, Y):\n",
    "            targets = torch.from_numpy(y).view(batch_size, -1)\n",
    "            #print(targets.size())\n",
    "            h = tuple([each.data for each in h])\n",
    "            opt.zero_grad()  \n",
    "            output, h = net(x, h)\n",
    "            #print(output.size())\n",
    "            \n",
    "            loss = criterion(output, targets.long())\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "        \"Train Loss: {:.4f}...\".format(np.mean(train_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(net, x, y, batch_size = 1):\n",
    "    with torch.no_grad():\n",
    "        h = net.init_hidden(batch_size)\n",
    "        output, h = net(x, h)\n",
    "        output = output.squeeze()\n",
    "    label = []\n",
    "    for i in range(len(output)):\n",
    "        if(output[i,0] >= output[i,1]):\n",
    "            label.append(0)\n",
    "        else:\n",
    "            label.append(1)\n",
    "    label = np.asarray(label)\n",
    "    acc = len(y[label == y])/len(y)\n",
    "    return acc\n",
    "\n",
    "def check_performance(net, X, Y):\n",
    "    acc_list = []\n",
    "    for i in range(X.shape[0]):\n",
    "        acc = compute_accuracy(net, X[i], Y[i])\n",
    "        acc_list.append(acc)\n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onerun(net, path):\n",
    "        print(\"creating synthetic data\")\n",
    "        trainfile,testfile = train_test_split(path)\n",
    "        train_data1, train_dict1 = mix_data(path, trainfile, 5, 30) \n",
    "        train_data2, train_dict2 = mix_data(path, trainfile, 5, 20)\n",
    "        train_data3, train_dict3 = mix_data(path, trainfile, 5, 15) \n",
    "        train_data4, train_dict4 = mix_data(path, trainfile, 5, 10)\n",
    "        test_data1, test_dict1 = mix_data(path, testfile, 5, 30) \n",
    "        test_data2, test_dict2 = mix_data(path, testfile, 5, 20)\n",
    "        test_data3, test_dict3 = mix_data(path, testfile, 5, 15) \n",
    "        test_data4, test_dict4 = mix_data(path, testfile, 5, 10) \n",
    "        print(\"smoothing data\")\n",
    "        train_data1 = smooth(train_data1, train_dict1)\n",
    "        train_data2 = smooth(train_data2, train_dict2)\n",
    "        train_data3 = smooth(train_data3, train_dict3)\n",
    "        train_data4 = smooth(train_data4, train_dict4)\n",
    "        test_data1 = smooth(test_data1, test_dict1)\n",
    "        test_data2 = smooth(test_data2, test_dict2)\n",
    "        test_data3 = smooth(test_data3, test_dict3)\n",
    "        test_data4 = smooth(test_data4, test_dict4)\n",
    "        print(\"split datasets\")\n",
    "        original_trainX, original_trainY = load_original_data(path,trainfile,12000)\n",
    "        original_testX, original_testY = load_original_data(path,testfile,12000)\n",
    "        X1_train, Y1_train = make_fake_dataset(train_data1, train_dict1)\n",
    "        X2_train, Y2_train = make_fake_dataset(train_data2, train_dict2)\n",
    "        X3_train, Y3_train = make_fake_dataset(train_data3, train_dict3)\n",
    "        X4_train, Y4_train = make_fake_dataset(train_data4, train_dict4)\n",
    "        X1_test, Y1_test = make_fake_dataset(test_data1, test_dict1)\n",
    "        X2_test, Y2_test = make_fake_dataset(test_data2, test_dict2)\n",
    "        X3_test, Y3_test = make_fake_dataset(test_data3, test_dict3)\n",
    "        X4_test, Y4_test = make_fake_dataset(test_data4, test_dict4)\n",
    "        train_X = np.vstack([X1_train, X2_train, X3_train, original_trainX, X4_train])\n",
    "        train_Y = np.vstack([Y1_train, Y2_train, Y3_train, original_trainY, Y4_train])\n",
    "        test_X = np.vstack([X1_test, X2_test, X3_test, original_testX, X4_test])\n",
    "        test_Y = np.vstack([Y1_test, Y2_test, Y3_test, original_testY, Y4_test])\n",
    "        #data preparing complete!\n",
    "        train_batch_m2m(net, train_X, train_Y)\n",
    "        acc_list1 = check_performance(net, X1_test, Y1_test)\n",
    "        acc_list2 = check_performance(net, X2_test, Y2_test)\n",
    "        acc_list3 = check_performance(net, X3_test, Y3_test)\n",
    "        acc_list4 = check_performance(net, X4_test, Y4_test)\n",
    "        acc_list5 = check_performance(net, original_testX, original_testY)\n",
    "        return acc_list1, acc_list2, acc_list3, acc_list4, acc_list5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating synthetic data\n",
      "smoothing data\n",
      "split datasets\n",
      "Epoch: 1/20... Train Loss: 0.6020...\n",
      "Epoch: 2/20... Train Loss: 0.5864...\n",
      "Epoch: 3/20... Train Loss: 0.5674...\n",
      "Epoch: 4/20... Train Loss: 0.5465...\n",
      "Epoch: 5/20... Train Loss: 0.5294...\n",
      "Epoch: 6/20... Train Loss: 0.5023...\n",
      "Epoch: 7/20... Train Loss: 0.4951...\n",
      "Epoch: 8/20... Train Loss: 0.4927...\n",
      "Epoch: 9/20... Train Loss: 0.4921...\n",
      "Epoch: 10/20... Train Loss: 0.4929...\n",
      "Epoch: 11/20... Train Loss: 0.4899...\n",
      "Epoch: 12/20... Train Loss: 0.4885...\n",
      "Epoch: 13/20... Train Loss: 0.4905...\n",
      "Epoch: 14/20... Train Loss: 0.4904...\n",
      "Epoch: 15/20... Train Loss: 0.4891...\n",
      "Epoch: 16/20... Train Loss: 0.4909...\n",
      "Epoch: 17/20... Train Loss: 0.4866...\n",
      "Epoch: 18/20... Train Loss: 0.4876...\n",
      "Epoch: 19/20... Train Loss: 0.4882...\n",
      "Epoch: 20/20... Train Loss: 0.4864...\n"
     ]
    }
   ],
   "source": [
    "net = cnn_lstm1()\n",
    "net.apply(init_weights1)\n",
    "acc_list1, acc_list2, acc_list3, acc_list4, acc_list5 = onerun(net, fixlen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7042222222222222\n",
      "0.8233333333333335\n",
      "0.8793333333333332\n",
      "0.9146666666666667\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(acc_list1))\n",
    "print(np.mean(acc_list2))\n",
    "print(np.mean(acc_list3))\n",
    "print(np.mean(acc_list4))\n",
    "print(np.mean(acc_list5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
