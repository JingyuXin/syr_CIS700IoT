{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is used to do many-to-one prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "fixlen_path = '/home/jxin05/BBMAS_Data/fixlen_data/' # data with length 2mins is in this folder, each sample is a .csv file\n",
    "\n",
    "def train_test_split(path): #split data into 80/20 as training and testing set\n",
    "    file_list = os.listdir(path)\n",
    "    idx = np.random.permutation(len(file_list))\n",
    "    train_idx = idx[:int(len(file_list)*0.8)]\n",
    "    test_idx = idx[int(len(file_list)*0.8):]\n",
    "    trainfile = []\n",
    "    testfile = []\n",
    "    for i in train_idx:\n",
    "        trainfile.append(file_list[i])\n",
    "    for i in test_idx:\n",
    "        testfile.append(file_list[i])\n",
    "    return trainfile,testfile\n",
    "\n",
    "def load_original_data2(path,filelist, fix_len): \n",
    "    X = np.zeros((len(filelist), 3, fix_len))\n",
    "    Y = np.zeros((len(filelist),1))    \n",
    "    for i in range(len(filelist)):\n",
    "        x = remove_col(pd.read_csv(path+filelist[i])).values\n",
    "        X[i,:,:] = x.T\n",
    "    return X,Y # X in shape of (num_sample, 3, 12000), Y is a vector if 0s\n",
    "\n",
    "def label_fromList(Xlist):\n",
    "    num_sample = len(Xlist)\n",
    "    Y = np.ones((num_sample,1))\n",
    "    X = np.zeros((num_sample, Xlist[0].shape[1], Xlist[0].shape[0]))\n",
    "    for i in range(num_sample):\n",
    "        X[i, :, :] = Xlist[i].T\n",
    "    return X,Y\n",
    "\n",
    "def remove_col(df):\n",
    "    return df.drop(['EID','time','time_in_ms'], axis =1)\n",
    "\n",
    "# filelist can be list of train files or test files\n",
    "# fake_number is the number of synthetic samples to be generated\n",
    "# replace_time is the number of parts to be replaced in a pure sample\n",
    "def mix_data(path, filelist, fake_number, replace_time, replace_size = 200):\n",
    "    pick_dict = {}\n",
    "    mixed_data = []\n",
    "    count = 0\n",
    "    while(count < fake_number):\n",
    "        pick = np.random.randint(0, len(filelist)) # index of pure data to be processed\n",
    "        pick_dict[count] = []\n",
    "        #print(\"creating fake sample from \" + filelist[pick])\n",
    "        target = pd.read_csv(path + filelist[pick])\n",
    "        mix = np.asarray(remove_col(target).values)\n",
    "        for j in range(replace_time):\n",
    "            start = np.random.randint(3,len(target)/replace_size-1)\n",
    "            while(start in pick_dict[count]):\n",
    "                start = np.random.randint(3,len(target)/replace_size-1)\n",
    "            pick_dict[count].append(start)\n",
    "            replace_clip = take_from_others(path, filelist,replace_size, pick)\n",
    "            mix[start*replace_size:(start+1)*replace_size, :] = replace_clip\n",
    "        mixed_data.append(mix)\n",
    "        #print(\"clips replaced: {}\".format(np.sort(pick_dict[count])))\n",
    "        count+=1\n",
    "        #print(\"**********\")\n",
    "    return mixed_data, pick_dict \n",
    "\n",
    "# randomly choose a part of signal whose length is replace_size from a file rather than the pick file\n",
    "def take_from_others(path, filelist, replace_size, pick):\n",
    "    takefrom = np.random.randint(0,len(filelist))\n",
    "    while(takefrom == pick):\n",
    "        takefrom = np.random.randint(0,len(filelist))\n",
    "    target_df = pd.read_csv(path+filelist[takefrom])\n",
    "    #print(filelist[takefrom])\n",
    "    #print(len(target_df))\n",
    "    start_idx = np.random.randint(0,len(target_df)-replace_size)\n",
    "    replace_clip = target_df.iloc[start_idx:start_idx+replace_size]\n",
    "    replace_clip = remove_col(replace_clip)\n",
    "    return np.asarray(replace_clip.values)\n",
    "\n",
    "# apply exponential moving average on the boundary between pure signal and replaced signal \n",
    "def smooth(mixed_data, pick_dict, window = 3):\n",
    "    res = mixed_data\n",
    "    for i in range(len(mixed_data)):\n",
    "        for v in pick_dict[i]:\n",
    "            start = v*200-10\n",
    "            end = v*200+10\n",
    "            data_x = mixed_data[i][start:end,0]\n",
    "            data_y = mixed_data[i][start:end,1]\n",
    "            data_z = mixed_data[i][start:end,2]\n",
    "            smoothed_x = ExpMovingAverage(data_x, window)\n",
    "            smoothed_y = ExpMovingAverage(data_y, window)\n",
    "            smoothed_z = ExpMovingAverage(data_z, window)\n",
    "            res[i][start+window+1:end-window,0] = smoothed_x[window+1:-window]\n",
    "            res[i][start+window+1:end-window,1] = smoothed_y[window+1:-window]\n",
    "            res[i][start+window+1:end-window,2] = smoothed_z[window+1:-window]\n",
    "    return res\n",
    "\n",
    "def ExpMovingAverage(array, window):\n",
    "    weights = np.exp(np.linspace(-1., 0., window))\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    a = np.convolve(array, weights, mode='full')[:len(array)]\n",
    "    a[:window] = a[window]\n",
    "    return a\n",
    "\n",
    "# transformate input X to fit the neural network \n",
    "def transformation(x):\n",
    "    if(len(x.shape) == 3): # train with batch\n",
    "        inputslist = []\n",
    "        for i in range(x.shape[0]):\n",
    "            inputs_x = torch.from_numpy(x[i,0,:]).float()\n",
    "            inputs_x = inputs_x.view(60,-1)\n",
    "            inputs_y = torch.from_numpy(x[i,1,:]).float()\n",
    "            inputs_y = inputs_y.view(60,-1)\n",
    "            inputs_z = torch.from_numpy(x[i,2,:]).float()\n",
    "            inputs_z = inputs_z.view(60,-1)\n",
    "            inputs = torch.stack([inputs_x, inputs_y, inputs_z])\n",
    "            inputs = inputs.transpose(0,1)\n",
    "            inputslist.append(inputs)\n",
    "        res = torch.stack(inputslist)\n",
    "        res = res.view(x.shape[0], 60, 1, 3, 200)\n",
    "        return res\n",
    "    else: # train with one sample\n",
    "        inputs_x = torch.from_numpy(x[0,:]).float()\n",
    "        inputs_x = inputs_x.view(60,-1)\n",
    "        inputs_y = torch.from_numpy(x[1,:]).float()\n",
    "        inputs_y = inputs_y.view(60,-1)\n",
    "        inputs_z = torch.from_numpy(x[2,:]).float()\n",
    "        inputs_z = inputs_z.view(60,-1)\n",
    "        inputs = torch.stack([inputs_x, inputs_y, inputs_z])\n",
    "        inputs = inputs.transpose(0,1)\n",
    "        inputs = inputs.view(-1, 60, 1, 3, 200)\n",
    "        return inputs\n",
    "\n",
    "#generate mini-batches\n",
    "def take_batch(batch_size, X, Y):\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    for i in range(0,X.shape[0]-batch_size+1, batch_size):\n",
    "        excerpt = indices[i:i + batch_size]\n",
    "        yield X[excerpt], Y[excerpt]\n",
    "\n",
    "def train_batch_m2one(net, X,Y, batch_size = 20, epochs = 25, lr = 0.01):\n",
    "    opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(weight= torch.tensor([1-(125/900), 125/900]))\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        train_loss = []\n",
    "        net.train()\n",
    "        for x,y in take_batch(batch_size, X, Y):\n",
    "            targets = torch.from_numpy(y).view(batch_size, -1)\n",
    "            targets = targets.squeeze()\n",
    "            #print(targets.size())\n",
    "            h = tuple([each.data for each in h])\n",
    "            opt.zero_grad()  \n",
    "            output, h = net(x, h)\n",
    "            output = output[:,:,-1]\n",
    "            #print(output.size())\n",
    "            loss = criterion(output, targets.long())\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "        \"Train Loss: {:.4f}...\".format(np.mean(train_loss)))\n",
    "        \n",
    "def predict(net, x):\n",
    "    with torch.no_grad():\n",
    "        h = net.init_hidden(1)\n",
    "        output, h = net(x,h)\n",
    "    if(output[0,-1,0] >= output[0, -1, 1]):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def check_acc(net, X, Y):\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        res = predict(net,X[i])\n",
    "        if(Y[i] == res):\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    return correct/X.shape[0]\n",
    "        \n",
    "def onerun2(net, path):\n",
    "    print(\"creating synthetic data\")\n",
    "    trainfile,testfile = train_test_split(path)\n",
    "    train_data1, train_dict1 = mix_data(path, trainfile, 225, 30) \n",
    "    train_data2, train_dict2 = mix_data(path, trainfile, 225, 20)\n",
    "    train_data3, train_dict3 = mix_data(path, trainfile, 225, 15) \n",
    "    train_data4, train_dict4 = mix_data(path, trainfile, 225, 10) \n",
    "    test_data1, test_dict1 = mix_data(path, testfile, 75, 30) \n",
    "    test_data2, test_dict2 = mix_data(path, testfile, 75, 20)\n",
    "    test_data3, test_dict3 = mix_data(path, testfile, 75, 15) \n",
    "    test_data4, test_dict4 = mix_data(path, testfile, 75, 10) \n",
    "    print(\"smoothing data\")\n",
    "    train_data1 = smooth(train_data1, train_dict1)\n",
    "    train_data2 = smooth(train_data2, train_dict2)\n",
    "    train_data3 = smooth(train_data3, train_dict3)\n",
    "    train_data4 = smooth(train_data4, train_dict4)\n",
    "    test_data1 = smooth(test_data1, test_dict1)\n",
    "    test_data2 = smooth(test_data2, test_dict2)\n",
    "    test_data3 = smooth(test_data3, test_dict3)\n",
    "    test_data4 = smooth(test_data4, test_dict4)\n",
    "    X1_train, Y1_train = label_fromList(train_data1)\n",
    "    X2_train, Y2_train = label_fromList(train_data2)\n",
    "    X3_train, Y3_train = label_fromList(train_data3)\n",
    "    X4_train, Y4_train = label_fromList(train_data4)\n",
    "    print(\"split datasets\")\n",
    "    original_trainX, original_trainY = load_original_data2(path,trainfile, 12000)\n",
    "    X1_test, Y1_test = label_fromList(test_data1)\n",
    "    X2_test, Y2_test = label_fromList(test_data2)\n",
    "    X3_test, Y3_test = label_fromList(test_data3)\n",
    "    X4_test, Y4_test = label_fromList(test_data4)\n",
    "    original_testX, original_testY = load_original_data2(path,testfile, 12000)\n",
    "    train_X = np.vstack([X1_train, X2_train, X3_train, original_trainX, X4_train])\n",
    "    train_Y = np.vstack([Y1_train, Y2_train, Y3_train, original_trainY, Y4_train])\n",
    "    test_X = np.vstack([X1_test, X2_test, X3_test, original_testX, X4_test])\n",
    "    test_Y = np.vstack([Y1_test, Y2_test, Y3_test, original_testY, Y4_test])\n",
    "    \n",
    "    train_batch_m2one(net, train_X, train_Y, epochs=20)\n",
    "    acc1 = check_acc(net, X1_test, Y1_test)\n",
    "    acc2 = check_acc(net, X2_test, Y2_test)\n",
    "    acc3 = check_acc(net, X3_test, Y3_test)\n",
    "    acc4 = check_acc(net, X1_test, Y4_test)\n",
    "    acc5 = check_acc(net, original_testX, original_testY)\n",
    "    return acc1, acc2, acc3, acc4, acc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cnn_lstm2(nn.Module):\n",
    "    \n",
    "    def __init__(self,drop_prob = 0.3, n_class = 2, n_layer = 1):\n",
    "        super(cnn_lstm2, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,32,(2,8), stride = (1,4))\n",
    "        self.conv2 = nn.Conv2d(32,64, (2,4), stride = (1,2))\n",
    "        self.conv3 = nn.Conv1d(64,32, 4, stride = 2)\n",
    "        self.lstm = nn.LSTM(320, 128, num_layers = 1, batch_first = True)\n",
    "        self.fc1 = nn.Linear(128, 32)\n",
    "        self.fc2 = nn.Linear(32,n_class)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.outlayer = nn.Softmax(dim = 2)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        inputs = transformation(x)\n",
    "        batch_size, timesteps, C, H, W = inputs.size()\n",
    "        inputs = inputs.view(batch_size*timesteps,C,H,W)\n",
    "        inputs = self.conv1(inputs)\n",
    "        inputs = F.relu(inputs)\n",
    "        inputs = self.conv2(inputs)\n",
    "        inputs = F.relu(inputs)\n",
    "        inputs = inputs.squeeze()\n",
    "        inputs = self.conv3(inputs)\n",
    "        inputs = F.relu(inputs)\n",
    "        \n",
    "        inputs = inputs.view(batch_size, timesteps, -1)\n",
    "        inputs, hidden = self.lstm(inputs,hidden)\n",
    "        inputs = self.dropout(inputs)\n",
    "        inputs = self.fc1(inputs)\n",
    "        inputs = self.dropout(inputs)\n",
    "        inputs = self.fc2(inputs)\n",
    "        out = self.outlayer(inputs)\n",
    "        \n",
    "        if(batch_size != 1):\n",
    "            #print(\"doing transpose\")\n",
    "            out = out.transpose(1,2)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(1, batch_size, 128).zero_(),\n",
    "                      weight.new(1, batch_size, 128).zero_())\n",
    "        return hidden\n",
    "\n",
    "def init_weights1(m):\n",
    "    if type(m) == nn.LSTM:\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    elif type(m) == nn.Conv2d or type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating synthetic data\n",
      "smoothing data\n",
      "split datasets\n",
      "Epoch: 1/20... Train Loss: 0.6860...\n",
      "Epoch: 2/20... Train Loss: 0.6371...\n",
      "Epoch: 3/20... Train Loss: 0.6043...\n",
      "Epoch: 4/20... Train Loss: 0.5963...\n",
      "Epoch: 5/20... Train Loss: 0.5896...\n",
      "Epoch: 6/20... Train Loss: 0.5567...\n",
      "Epoch: 7/20... Train Loss: 0.5809...\n",
      "Epoch: 8/20... Train Loss: 0.5002...\n",
      "Epoch: 9/20... Train Loss: 0.4719...\n",
      "Epoch: 10/20... Train Loss: 0.5585...\n",
      "Epoch: 11/20... Train Loss: 0.4750...\n",
      "Epoch: 12/20... Train Loss: 0.4683...\n",
      "Epoch: 13/20... Train Loss: 0.5336...\n",
      "Epoch: 14/20... Train Loss: 0.5045...\n",
      "Epoch: 15/20... Train Loss: 0.6540...\n",
      "Epoch: 16/20... Train Loss: 0.6358...\n",
      "Epoch: 17/20... Train Loss: 0.6188...\n",
      "Epoch: 18/20... Train Loss: 0.6726...\n",
      "Epoch: 19/20... Train Loss: 0.7022...\n",
      "Epoch: 20/20... Train Loss: 0.6153...\n"
     ]
    }
   ],
   "source": [
    "net2 = cnn_lstm2()\n",
    "net2.apply(init_weights1)\n",
    "acc1, acc2, acc3, acc4, acc5 = onerun2(net2,fixlen_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "0.84\n",
      "0.72\n",
      "0.96\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(acc1)\n",
    "print(acc2)\n",
    "print(acc3)\n",
    "print(acc4)\n",
    "print(acc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
